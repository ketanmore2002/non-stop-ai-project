{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The news website used for scraping the articles is washingtonpost.com. This website consist of 4 important sections (politics,style,climate,tech)","metadata":{}},{"cell_type":"markdown","source":"**Categories**\n1.  politics\n2.  style\n3.  climate\n3.  money","metadata":{}},{"cell_type":"markdown","source":"* We could use beautifulsoup for scraping articles from the websites but since beautifulsoup requires articles in stuctured format (HTML) this approch wont work for most of the websites\n* I have use a python Newspaper3k for scraping articles from the websites. This library does not require articles in stuctured format.","metadata":{}},{"cell_type":"markdown","source":"### For all the 4 sections we are separately scraping the data ","metadata":{}},{"cell_type":"code","source":"!pip3 install newspaper3k","metadata":{"id":"WuYoCGaz0Dvw","outputId":"37f6ce4a-86f7-4fd7-c183-40d55a7aff3c","execution":{"iopub.status.busy":"2023-11-22T21:59:42.251413Z","iopub.execute_input":"2023-11-22T21:59:42.252084Z","iopub.status.idle":"2023-11-22T22:00:08.446891Z","shell.execute_reply.started":"2023-11-22T21:59:42.252056Z","shell.execute_reply":"2023-11-22T22:00:08.445712Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')","metadata":{"id":"1WyDisc4MS1u","outputId":"6e6ccc50-66ab-4770-8c5a-c4bda06cbf21","execution":{"iopub.status.busy":"2023-11-22T22:00:08.449265Z","iopub.execute_input":"2023-11-22T22:00:08.450032Z","iopub.status.idle":"2023-11-22T22:00:10.266571Z","shell.execute_reply.started":"2023-11-22T22:00:08.449990Z","shell.execute_reply":"2023-11-22T22:00:10.265660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Business Data Scrapping\n---\n\n","metadata":{"id":"j67Sj6iFNF0y"}},{"cell_type":"code","source":"import newspaper\nimport pandas as pd\nfrom tqdm import tqdm\n\nuser_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\nnews_paper = newspaper.build(\"https://www.washingtonpost.com/politics/\", memoize_articles=False, user_agent=user_agent)\n\n\n\nl = []\nfor article in tqdm(news_paper.articles[:200], desc=\"Processing articles\"):\n    article.download()\n    article.parse()\n    article.nlp()\n    keywords = ' '.join(article.keywords)\n    l.append({\"Title\": article.title, \"Category\": 1, \"Date\": article.publish_date, \"Keywords\": keywords, \"URL\": article.url})\n\ndf = pd.DataFrame(l)\ndf.to_csv(\"politics.csv\")","metadata":{"id":"CsBu1fd-DKba","outputId":"3d1db037-7c68-4683-f06e-37f07ecb5b8c","execution":{"iopub.status.busy":"2023-11-22T22:00:10.268107Z","iopub.execute_input":"2023-11-22T22:00:10.268868Z","iopub.status.idle":"2023-11-22T22:01:29.430168Z","shell.execute_reply.started":"2023-11-22T22:00:10.268829Z","shell.execute_reply":"2023-11-22T22:01:29.429276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tech Data Scraping\n---\n\n","metadata":{"id":"kdGZt5GHNcUT"}},{"cell_type":"code","source":"user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\nnews_paper = newspaper.build(\"https://www.washingtonpost.com/style/\", memoize_articles=False, user_agent=user_agent)\n\n\n\nl = []\nfor article in tqdm(news_paper.articles[:200], desc=\"Processing articles\"):\n    article.download()\n    article.parse()\n    article.nlp()\n    keywords = ' '.join(article.keywords)\n    l.append({\"Title\": article.title, \"Category\": 2, \"Date\": article.publish_date, \"Keywords\": keywords, \"URL\": article.url})\n\ndf = pd.DataFrame(l)\ndf.to_csv(\"tech.csv\")","metadata":{"id":"s9z0Z2nnM5an","outputId":"9cad632e-c223-4b87-dde8-e7e04adf5bec","execution":{"iopub.status.busy":"2023-11-22T22:01:29.432224Z","iopub.execute_input":"2023-11-22T22:01:29.432520Z","iopub.status.idle":"2023-11-22T22:02:26.557930Z","shell.execute_reply.started":"2023-11-22T22:01:29.432493Z","shell.execute_reply":"2023-11-22T22:02:26.556892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Market Data Scraping\n---\n\n","metadata":{"id":"FFnsBhIGNy6v"}},{"cell_type":"code","source":"user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\nnews_paper = newspaper.build(\"https://www.washingtonpost.com/climate-environment/\", memoize_articles=False, user_agent=user_agent)\n\n\n\nl = []\nfor article in tqdm(news_paper.articles[:200], desc=\"Processing articles\"):\n    article.download()\n    article.parse()\n    article.nlp()\n    keywords = ' '.join(article.keywords)\n    l.append({\"Title\": article.title, \"Category\": 3, \"Date\": article.publish_date, \"Keywords\": keywords, \"URL\": article.url})\n\ndf = pd.DataFrame(l)\ndf.to_csv(\"climate.csv\")","metadata":{"id":"nrkQBqKcNrLy","outputId":"9c7984fc-b2af-4e77-ed2b-02c45602838e","execution":{"iopub.status.busy":"2023-11-22T22:02:26.559035Z","iopub.execute_input":"2023-11-22T22:02:26.559311Z","iopub.status.idle":"2023-11-22T22:03:15.838402Z","shell.execute_reply.started":"2023-11-22T22:02:26.559286Z","shell.execute_reply":"2023-11-22T22:03:15.837474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reviews Data Scraping \n---\n\n","metadata":{"id":"Uqj2pEMTODRi"}},{"cell_type":"code","source":"user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\nnews_paper = newspaper.build(\"https://www.washingtonpost.com/business/technology/\", memoize_articles=False, user_agent=user_agent)\n\n\n\nl = []\nfor article in tqdm(news_paper.articles[:200], desc=\"Processing articles\"):\n    try :\n        article.download()\n        article.parse()\n        article.nlp()\n        keywords = ' '.join(article.keywords)\n        l.append({\"Title\": article.title, \"Category\": 4, \"Date\": article.publish_date, \"Keywords\": keywords, \"URL\": article.url})\n    except:\n        pass\n        \ndf = pd.DataFrame(l)\ndf.to_csv(\"technology.csv\")","metadata":{"id":"U0UIYdlVOiKH","outputId":"2f5bf176-34b2-4a80-90d4-cc367c48dc62","execution":{"iopub.status.busy":"2023-11-22T22:03:15.839733Z","iopub.execute_input":"2023-11-22T22:03:15.840116Z","iopub.status.idle":"2023-11-22T22:04:06.205894Z","shell.execute_reply.started":"2023-11-22T22:03:15.840081Z","shell.execute_reply":"2023-11-22T22:04:06.204999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing\n---\n\nIn this Step we are concatenating 4 dataframes we have creating will scraping data for 4 different sections.\nAlso dublicate values will be removed.","metadata":{"id":"GopcLj9jY70F"}},{"cell_type":"code","source":"file1_path = '/kaggle/working/politics.csv'\nfile2_path = '/kaggle/working/tech.csv'\nfile3_path = '/kaggle/working/climate.csv'\nfile4_path = '/kaggle/working/technology.csv'\n\ndf1 = pd.read_csv(file1_path)\ndf2 = pd.read_csv(file2_path)\ndf3 = pd.read_csv(file3_path)\ndf4 = pd.read_csv(file4_path)\n\ncombined_df = pd.concat([df1, df2, df3, df4], ignore_index=True)\ncombined_df.to_csv('combined_file_lite.csv', index=False)\n","metadata":{"id":"kCNRkaiSXa-5","execution":{"iopub.status.busy":"2023-11-22T22:04:06.206903Z","iopub.execute_input":"2023-11-22T22:04:06.207180Z","iopub.status.idle":"2023-11-22T22:04:06.235609Z","shell.execute_reply.started":"2023-11-22T22:04:06.207155Z","shell.execute_reply":"2023-11-22T22:04:06.234861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"combined_file_lite.csv\")\ndf = df[[\"Title\", \"Category\", \"Keywords\"]]\ndf.tail()","metadata":{"id":"ERu7c48AaJOc","outputId":"51d62e5e-4386-4e64-832d-8f7f40d9c3dc","execution":{"iopub.status.busy":"2023-11-22T22:04:06.236813Z","iopub.execute_input":"2023-11-22T22:04:06.237163Z","iopub.status.idle":"2023-11-22T22:04:06.264797Z","shell.execute_reply.started":"2023-11-22T22:04:06.237128Z","shell.execute_reply":"2023-11-22T22:04:06.264002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop_duplicates()\ndf.tail()","metadata":{"id":"3jBvL_nZ-JP8","outputId":"49714498-84b7-4bea-cb01-ace92894a76d","execution":{"iopub.status.busy":"2023-11-22T22:04:06.265733Z","iopub.execute_input":"2023-11-22T22:04:06.266055Z","iopub.status.idle":"2023-11-22T22:04:06.281110Z","shell.execute_reply.started":"2023-11-22T22:04:06.266029Z","shell.execute_reply":"2023-11-22T22:04:06.280101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"combined_file.csv\")","metadata":{"id":"xc17YpTgBA0G","execution":{"iopub.status.busy":"2023-11-22T22:04:06.286942Z","iopub.execute_input":"2023-11-22T22:04:06.287228Z","iopub.status.idle":"2023-11-22T22:04:06.297789Z","shell.execute_reply.started":"2023-11-22T22:04:06.287202Z","shell.execute_reply":"2023-11-22T22:04:06.297002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building ML Model\n\n---\n* After testing this complex data set with big model like bert, I found that I wont be able to complete training due to lack of computation power. As bert is heavy model.\n* So I decided to move with Logistic Regression. For this \n1. Stop words were removed.\n2. data was tokenized.\n3. Then, data was embedded with the help of all-MiniLM-L6-v2 model from Transformer.","metadata":{"id":"hCSNwdtUevt4"}},{"cell_type":"code","source":"!pip install -U sentence-transformers -q","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:06.298723Z","iopub.execute_input":"2023-11-22T22:04:06.299016Z","iopub.status.idle":"2023-11-22T22:04:20.800847Z","shell.execute_reply.started":"2023-11-22T22:04:06.298992Z","shell.execute_reply":"2023-11-22T22:04:20.799716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:20.802739Z","iopub.execute_input":"2023-11-22T22:04:20.803151Z","iopub.status.idle":"2023-11-22T22:04:30.520243Z","shell.execute_reply.started":"2023-11-22T22:04:20.803112Z","shell.execute_reply":"2023-11-22T22:04:30.519394Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/working/combined_file.csv\" ,engine=\"python\")\ndata.tail()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:30.521447Z","iopub.execute_input":"2023-11-22T22:04:30.521964Z","iopub.status.idle":"2023-11-22T22:04:30.539610Z","shell.execute_reply.started":"2023-11-22T22:04:30.521936Z","shell.execute_reply":"2023-11-22T22:04:30.538485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport string\nnlp = spacy.load(\"en_core_web_sm\")\nstop_words = nlp.Defaults.stop_words","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:30.540841Z","iopub.execute_input":"2023-11-22T22:04:30.541269Z","iopub.status.idle":"2023-11-22T22:04:35.015201Z","shell.execute_reply.started":"2023-11-22T22:04:30.541241Z","shell.execute_reply":"2023-11-22T22:04:35.014166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punctuations = string.punctuation\nprint(punctuations)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:35.016342Z","iopub.execute_input":"2023-11-22T22:04:35.016915Z","iopub.status.idle":"2023-11-22T22:04:35.022686Z","shell.execute_reply.started":"2023-11-22T22:04:35.016884Z","shell.execute_reply":"2023-11-22T22:04:35.021524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def spacy_tokenizer(sentence):\n    doc = nlp(sentence)\n    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n    sentence = \" \".join(mytokens)\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:35.024185Z","iopub.execute_input":"2023-11-22T22:04:35.024570Z","iopub.status.idle":"2023-11-22T22:04:35.034716Z","shell.execute_reply.started":"2023-11-22T22:04:35.024536Z","shell.execute_reply":"2023-11-22T22:04:35.033827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['tokenize'] = data['Keywords'].apply(spacy_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:35.035847Z","iopub.execute_input":"2023-11-22T22:04:35.036399Z","iopub.status.idle":"2023-11-22T22:04:39.395057Z","shell.execute_reply.started":"2023-11-22T22:04:35.036371Z","shell.execute_reply":"2023-11-22T22:04:39.394184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:39.396190Z","iopub.execute_input":"2023-11-22T22:04:39.396519Z","iopub.status.idle":"2023-11-22T22:04:39.407823Z","shell.execute_reply.started":"2023-11-22T22:04:39.396491Z","shell.execute_reply":"2023-11-22T22:04:39.406820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['embeddings'] = data['tokenize'].apply(model.encode)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:39.409012Z","iopub.execute_input":"2023-11-22T22:04:39.409274Z","iopub.status.idle":"2023-11-22T22:04:58.436580Z","shell.execute_reply.started":"2023-11-22T22:04:39.409250Z","shell.execute_reply":"2023-11-22T22:04:58.435648Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data['embeddings'].to_list()\ny = data['Category'].to_list()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:58.437822Z","iopub.execute_input":"2023-11-22T22:04:58.438121Z","iopub.status.idle":"2023-11-22T22:04:58.442725Z","shell.execute_reply.started":"2023-11-22T22:04:58.438094Z","shell.execute_reply":"2023-11-22T22:04:58.441794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:58.444086Z","iopub.execute_input":"2023-11-22T22:04:58.444721Z","iopub.status.idle":"2023-11-22T22:04:58.472084Z","shell.execute_reply.started":"2023-11-22T22:04:58.444687Z","shell.execute_reply":"2023-11-22T22:04:58.471100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:58.473700Z","iopub.execute_input":"2023-11-22T22:04:58.474598Z","iopub.status.idle":"2023-11-22T22:04:58.482659Z","shell.execute_reply.started":"2023-11-22T22:04:58.474561Z","shell.execute_reply":"2023-11-22T22:04:58.481703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation\n---","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:58.484136Z","iopub.execute_input":"2023-11-22T22:04:58.484434Z","iopub.status.idle":"2023-11-22T22:04:58.489797Z","shell.execute_reply.started":"2023-11-22T22:04:58.484409Z","shell.execute_reply":"2023-11-22T22:04:58.488927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LogisticRegression()\nparam_grid = {\n    'penalty': ['l1', 'l2', 'elasticnet'],\n    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n    'max_iter': [50, 100, 200, 500 , 1000],  \n}\n\ngrid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best Parameters: \", grid_search.best_params_)\nprint(\"Best Score: \", grid_search.best_score_) ","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:04:58.491217Z","iopub.execute_input":"2023-11-22T22:04:58.491744Z","iopub.status.idle":"2023-11-22T22:06:26.990812Z","shell.execute_reply.started":"2023-11-22T22:04:58.491705Z","shell.execute_reply":"2023-11-22T22:06:26.989539Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 6: Evaluate on Test Set\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\n\n# Step 7: Evaluate Performance\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\nprint(accuracy)\nprint(\"Classification Report:\\n\", report)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:06:26.992317Z","iopub.execute_input":"2023-11-22T22:06:26.992660Z","iopub.status.idle":"2023-11-22T22:06:27.010440Z","shell.execute_reply.started":"2023-11-22T22:06:26.992629Z","shell.execute_reply":"2023-11-22T22:06:27.009487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib\nbest_model = grid_search.best_estimator_\njoblib.dump(best_model, 'best_logistic_regression_model.joblib')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:06:27.011619Z","iopub.execute_input":"2023-11-22T22:06:27.011945Z","iopub.status.idle":"2023-11-22T22:06:27.020950Z","shell.execute_reply.started":"2023-11-22T22:06:27.011916Z","shell.execute_reply":"2023-11-22T22:06:27.019997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df = pd.DataFrame(grid_search.cv_results_)\nresults_df.to_csv('grid_search_results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:06:27.022406Z","iopub.execute_input":"2023-11-22T22:06:27.022798Z","iopub.status.idle":"2023-11-22T22:06:27.045356Z","shell.execute_reply.started":"2023-11-22T22:06:27.022731Z","shell.execute_reply":"2023-11-22T22:06:27.044573Z"},"trusted":true},"execution_count":null,"outputs":[]}]}